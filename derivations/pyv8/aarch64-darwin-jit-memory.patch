diff --git a/src/a64/assembler-a64-inl.h b/src/a64/assembler-a64-inl.h
index e68dee07382..6d7829f7d49 100644
--- a/v8-source/src/a64/assembler-a64-inl.h
+++ b/v8-source/src/a64/assembler-a64-inl.h
@@ -618,7 +618,9 @@ void Assembler::deserialization_set_special_target_at(
 
 
 void Assembler::set_target_address_at(Address pc, Address target) {
+  VirtualMemory::SetJITWriteProtect(false);
   Memory::Address_at(target_pointer_address_at(pc)) = target;
+  VirtualMemory::SetJITWriteProtect(true);
   // Intuitively, we would think it is necessary to always flush the
   // instruction cache after patching a target address in the code as follows:
   //   CPU::FlushICache(pc, sizeof(target));
diff --git a/src/a64/assembler-a64.h b/src/a64/assembler-a64.h
index a2c93df2ae2..306d2acfb07 100644
--- a/v8-source/src/a64/assembler-a64.h
+++ b/v8-source/src/a64/assembler-a64.h
@@ -1883,9 +1883,11 @@ class Assembler : public AssemblerBase {
     STATIC_ASSERT(sizeof(instruction) == kInstructionSize);
     ASSERT((pc_ + sizeof(instruction)) <= (buffer_ + buffer_size_));
 
+    VirtualMemory::SetJITWriteProtect(false);
     memcpy(pc_, &instruction, sizeof(instruction));
     pc_ += sizeof(instruction);
     CheckBuffer();
+    VirtualMemory::SetJITWriteProtect(true);
   }
 
   // Emit data inline in the instruction stream.
@@ -1895,9 +1897,11 @@ class Assembler : public AssemblerBase {
 
     // TODO(all): Somehow register we have some data here. Then we can
     // disassemble it correctly.
+    VirtualMemory::SetJITWriteProtect(false);
     memcpy(pc_, data, size);
     pc_ += size;
     CheckBuffer();
+    VirtualMemory::SetJITWriteProtect(true);
   }
 
   void GrowBuffer();
diff --git a/src/execution.cc b/src/execution.cc
index 690a4e3f4e6..6cc01420a0d 100644
--- a/v8-source/src/execution.cc
+++ b/v8-source/src/execution.cc
@@ -103,6 +103,7 @@ static Handle<Object> Invoke(bool is_construct,
   // make the current one is indeed a global object.
   ASSERT(function->context()->global_object()->IsGlobalObject());
 
+  VirtualMemory::SetJITWriteProtect(true);
   {
     // Save and restore context around invocation and block the
     // allocation of handles without explicit handle scopes.
diff --git a/src/heap.cc b/src/heap.cc
index 42e56ca1eb9..adbf721165b 100644
--- a/v8-source/src/heap.cc
+++ b/v8-source/src/heap.cc
@@ -4010,6 +4010,8 @@ MaybeObject* Heap::CreateCode(const CodeDesc& desc,
                               bool immovable,
                               bool crankshafted,
                               int prologue_offset) {
+  VirtualMemory::SetJITWriteProtect(false);
+
   // Allocate ByteArray before the Code object, so that we do not risk
   // leaving uninitialized Code object (and breaking the heap).
   ByteArray* reloc_info;
@@ -4088,6 +4090,8 @@ MaybeObject* Heap::CreateCode(const CodeDesc& desc,
     code->Verify();
   }
 #endif
+
+  VirtualMemory::SetJITWriteProtect(true);
   return code;
 }
 
diff --git a/src/incremental-marking.cc b/src/incremental-marking.cc
index 1b9a28a5b7d..d2af2bb52f3 100644
--- a/v8-source/src/incremental-marking.cc
+++ b/v8-source/src/incremental-marking.cc
@@ -567,7 +567,8 @@ void IncrementalMarking::UncommitMarkingDeque() {
   if (state_ == STOPPED && marking_deque_memory_committed_) {
     bool success = marking_deque_memory_->Uncommit(
         reinterpret_cast<Address>(marking_deque_memory_->address()),
-        marking_deque_memory_->size());
+        marking_deque_memory_->size(),
+        false);  // Not executable.
     CHECK(success);
     marking_deque_memory_committed_ = false;
   }
diff --git a/src/mark-compact.cc b/src/mark-compact.cc
index f5504478036..d75ebfd6827 100644
--- a/v8-source/src/mark-compact.cc
+++ b/v8-source/src/mark-compact.cc
@@ -3250,6 +3250,7 @@ static void SweepPrecisely(PagedSpace* space,
     MarkBit::CellType* cell = it.CurrentCell();
     int live_objects = MarkWordToObjectStarts(*cell, offsets);
     int live_index = 0;
+    VirtualMemory::SetJITWriteProtect(false);
     for ( ; live_objects != 0; live_objects--) {
       Address free_end = cell_base + offsets[live_index++] * kPointerSize;
       if (free_end != free_start) {
@@ -3286,6 +3287,8 @@ static void SweepPrecisely(PagedSpace* space,
     // Clear marking bits for current cell.
     *cell = 0;
   }
+
+  VirtualMemory::SetJITWriteProtect(false);
   if (free_start != p->area_end()) {
     if (free_space_mode == ZAP_FREE_SPACE) {
       memset(free_start, 0xcc, static_cast<int>(p->area_end() - free_start));
@@ -3297,6 +3300,8 @@ static void SweepPrecisely(PagedSpace* space,
     }
 #endif
   }
+  VirtualMemory::SetJITWriteProtect(true);
+
   p->ResetLiveBytes();
   if (FLAG_print_cumulative_gc_stat) {
     space->heap()->AddSweepingTime(OS::TimeCurrentMillis() - start_time);
diff --git a/src/objects-inl.h b/src/objects-inl.h
index 15d9b3750ed..12b365d707e 100644
--- a/v8-source/src/objects-inl.h
+++ b/v8-source/src/objects-inl.h
@@ -1071,6 +1071,10 @@ MaybeObject* Object::GetProperty(Name* key, PropertyAttributes* attributes) {
   return GetPropertyWithReceiver(this, key, attributes);
 }
 
+#define DISABLE_JIT_WRITE_PROTECT(code)     \
+  VirtualMemory::SetJITWriteProtect(false); \
+  code;                                     \
+  VirtualMemory::SetJITWriteProtect(true);
 
 #define FIELD_ADDR(p, offset) \
   (reinterpret_cast<byte*>(p) + offset - kHeapObjectTag)
@@ -1079,7 +1083,7 @@ MaybeObject* Object::GetProperty(Name* key, PropertyAttributes* attributes) {
   (*reinterpret_cast<Object**>(FIELD_ADDR(p, offset)))
 
 #define WRITE_FIELD(p, offset, value) \
-  (*reinterpret_cast<Object**>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<Object**>(FIELD_ADDR(p, offset)) = value))
 
 #define WRITE_BARRIER(heap, object, offset, value)                      \
   heap->incremental_marking()->RecordWrite(                             \
@@ -1140,43 +1144,43 @@ MaybeObject* Object::GetProperty(Name* key, PropertyAttributes* attributes) {
   (*reinterpret_cast<int*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_INT_FIELD(p, offset, value) \
-  (*reinterpret_cast<int*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<int*>(FIELD_ADDR(p, offset)) = value))
 
 #define READ_INTPTR_FIELD(p, offset) \
   (*reinterpret_cast<intptr_t*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_INTPTR_FIELD(p, offset, value) \
-  (*reinterpret_cast<intptr_t*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<intptr_t*>(FIELD_ADDR(p, offset)) = value))
 
 #define READ_UINT32_FIELD(p, offset) \
   (*reinterpret_cast<uint32_t*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_UINT32_FIELD(p, offset, value) \
-  (*reinterpret_cast<uint32_t*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<uint32_t*>(FIELD_ADDR(p, offset)) = value))
 
 #define READ_INT32_FIELD(p, offset) \
   (*reinterpret_cast<int32_t*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_INT32_FIELD(p, offset, value) \
-  (*reinterpret_cast<int32_t*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<int32_t*>(FIELD_ADDR(p, offset)) = value))
 
 #define READ_INT64_FIELD(p, offset) \
   (*reinterpret_cast<int64_t*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_INT64_FIELD(p, offset, value) \
-  (*reinterpret_cast<int64_t*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<int64_t*>(FIELD_ADDR(p, offset)) = value))
 
 #define READ_SHORT_FIELD(p, offset) \
   (*reinterpret_cast<uint16_t*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_SHORT_FIELD(p, offset, value) \
-  (*reinterpret_cast<uint16_t*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<uint16_t*>(FIELD_ADDR(p, offset)) = value))
 
 #define READ_BYTE_FIELD(p, offset) \
   (*reinterpret_cast<byte*>(FIELD_ADDR(p, offset)))
 
 #define WRITE_BYTE_FIELD(p, offset, value) \
-  (*reinterpret_cast<byte*>(FIELD_ADDR(p, offset)) = value)
+  DISABLE_JIT_WRITE_PROTECT((*reinterpret_cast<byte*>(FIELD_ADDR(p, offset)) = value))
 
 
 Object** HeapObject::RawField(HeapObject* obj, int byte_offset) {
diff --git a/src/objects.cc b/src/objects.cc
index 11c8ffcacdd..d216201f223 100644
--- a/v8-source/src/objects.cc
+++ b/v8-source/src/objects.cc
@@ -10368,6 +10368,8 @@ void Code::Relocate(intptr_t delta) {
 void Code::CopyFrom(const CodeDesc& desc) {
   ASSERT(Marking::Color(this) == Marking::WHITE_OBJECT);
 
+  VirtualMemory::SetJITWriteProtect(false);
+
   // copy code
   CopyBytes(instruction_start(), desc.buffer,
             static_cast<size_t>(desc.instr_size));
@@ -10414,6 +10416,8 @@ void Code::CopyFrom(const CodeDesc& desc) {
     }
   }
   CPU::FlushICache(instruction_start(), instruction_size());
+
+  VirtualMemory::SetJITWriteProtect(true);
 }
 
 
diff --git a/src/platform-macos.cc b/src/platform-macos.cc
index 683a04d3815..3ba579947e6 100644
--- a/v8-source/src/platform-macos.cc
+++ b/v8-source/src/platform-macos.cc
@@ -212,10 +212,15 @@ VirtualMemory::VirtualMemory(size_t size, size_t alignment)
   ASSERT(IsAligned(alignment, static_cast<intptr_t>(OS::AllocateAlignment())));
   size_t request_size = RoundUp(size + alignment,
                                 static_cast<intptr_t>(OS::AllocateAlignment()));
+  int flags = MAP_PRIVATE | MAP_ANON | MAP_NORESERVE;
+#ifdef V8_TARGET_ARCH_A64
+  flags |= MAP_JIT;
+#endif
+
   void* reservation = mmap(OS::GetRandomMmapAddr(),
                            request_size,
                            PROT_NONE,
-                           MAP_PRIVATE | MAP_ANON | MAP_NORESERVE,
+                           flags,
                            kMmapFd,
                            kMmapFdOffset);
   if (reservation == MAP_FAILED) return;
@@ -272,8 +277,8 @@ bool VirtualMemory::Commit(void* address, size_t size, bool is_executable) {
 }
 
 
-bool VirtualMemory::Uncommit(void* address, size_t size) {
-  return UncommitRegion(address, size);
+bool VirtualMemory::Uncommit(void* address, size_t size, bool is_executable) {
+  return UncommitRegion(address, size, is_executable);
 }
 
 
@@ -284,10 +289,15 @@ bool VirtualMemory::Guard(void* address) {
 
 
 void* VirtualMemory::ReserveRegion(size_t size) {
+  int flags = MAP_PRIVATE | MAP_ANON | MAP_NORESERVE;
+#ifdef V8_TARGET_ARCH_A64
+  flags |= MAP_JIT;
+#endif
+
   void* result = mmap(OS::GetRandomMmapAddr(),
                       size,
                       PROT_NONE,
-                      MAP_PRIVATE | MAP_ANON | MAP_NORESERVE,
+                      flags,
                       kMmapFd,
                       kMmapFdOffset);
 
@@ -301,25 +311,37 @@ bool VirtualMemory::CommitRegion(void* address,
                                  size_t size,
                                  bool is_executable) {
   int prot = PROT_READ | PROT_WRITE | (is_executable ? PROT_EXEC : 0);
-  if (MAP_FAILED == mmap(address,
-                         size,
-                         prot,
-                         MAP_PRIVATE | MAP_ANON | MAP_FIXED,
-                         kMmapFd,
-                         kMmapFdOffset)) {
-    return false;
+  int flags = MAP_PRIVATE | MAP_ANON | MAP_FIXED;
+
+#ifdef V8_TARGET_ARCH_A64
+  if (is_executable) {
+    return mprotect(address, size, prot) == 0;
+  } else {
+    return mmap(address, size, prot, flags, kMmapFd, kMmapFdOffset) != MAP_FAILED;
   }
-  return true;
+#else
+  return mmap(address, size, prot, flags, kMmapFd, kMmapFdOffset) != MAP_FAILED;
+#endif
 }
 
 
-bool VirtualMemory::UncommitRegion(void* address, size_t size) {
-  return mmap(address,
-              size,
-              PROT_NONE,
+bool VirtualMemory::UncommitRegion(void* address, size_t size, bool is_executable) {
+#ifdef V8_TARGET_ARCH_A64
+  if (is_executable) {
+    if (madvise(address, size, MADV_FREE) != 0) {
+      return false;
+    }
+    return mprotect(address, size, PROT_NONE) == 0;
+  } else {
+    return mmap(address, size, PROT_NONE,
+                MAP_PRIVATE | MAP_ANON | MAP_NORESERVE | MAP_FIXED,
+                kMmapFd, kMmapFdOffset) != MAP_FAILED;
+  }
+#else
+  return mmap(address, size, PROT_NONE,
               MAP_PRIVATE | MAP_ANON | MAP_NORESERVE | MAP_FIXED,
-              kMmapFd,
-              kMmapFdOffset) != MAP_FAILED;
+              kMmapFd, kMmapFdOffset) != MAP_FAILED;
+#endif
 }
 
 
@@ -332,4 +354,12 @@ bool VirtualMemory::HasLazyCommits() {
   return false;
 }
 
+void VirtualMemory::SetJITWriteProtect(bool prot) {
+#ifdef V8_TARGET_ARCH_A64
+  if (__builtin_available(macOS 11.0, *)) {
+    pthread_jit_write_protect_np(prot);
+  }
+#endif
+}
+
 } }  // namespace v8::internal
diff --git a/src/platform.h b/src/platform.h
index 8af90f1cb3e..e9536781256 100644
--- a/v8-source/src/platform.h
+++ b/v8-source/src/platform.h
@@ -158,6 +158,99 @@ inline intptr_t InternalGetExistingThreadLocal(intptr_t index) {
 
 #endif  // V8_NO_FAST_TLS
 
+// Represents and controls an area of reserved memory.
+// Control of the reserved memory can be assigned to another VirtualMemory
+// object by assignment or copy-contructing. This removes the reserved memory
+// from the original object.
+class VirtualMemory {
+ public:
+  // Empty VirtualMemory object, controlling no reserved memory.
+  VirtualMemory();
+
+  // Reserves virtual memory with size.
+  explicit VirtualMemory(size_t size);
+
+  // Reserves virtual memory containing an area of the given size that
+  // is aligned per alignment. This may not be at the position returned
+  // by address().
+  VirtualMemory(size_t size, size_t alignment);
+
+  // Releases the reserved memory, if any, controlled by this VirtualMemory
+  // object.
+  ~VirtualMemory();
+
+  // Returns whether the memory has been reserved.
+  bool IsReserved();
+
+  // Initialize or resets an embedded VirtualMemory object.
+  void Reset();
+
+  // Returns the start address of the reserved memory.
+  // If the memory was reserved with an alignment, this address is not
+  // necessarily aligned. The user might need to round it up to a multiple of
+  // the alignment to get the start of the aligned block.
+  void* address() {
+    ASSERT(IsReserved());
+    return address_;
+  }
+
+  // Returns the size of the reserved memory. The returned value is only
+  // meaningful when IsReserved() returns true.
+  // If the memory was reserved with an alignment, this size may be larger
+  // than the requested size.
+  size_t size() { return size_; }
+
+  // Commits real memory. Returns whether the operation succeeded.
+  bool Commit(void* address, size_t size, bool is_executable);
+
+  // Uncommit real memory.  Returns whether the operation succeeded.
+  bool Uncommit(void* address, size_t size, bool is_executable);
+
+  // Creates a single guard page at the given address.
+  bool Guard(void* address);
+
+  void Release() {
+    ASSERT(IsReserved());
+    // Notice: Order is important here. The VirtualMemory object might live
+    // inside the allocated region.
+    void* address = address_;
+    size_t size = size_;
+    Reset();
+    bool result = ReleaseRegion(address, size);
+    USE(result);
+    ASSERT(result);
+  }
+
+  // Assign control of the reserved region to a different VirtualMemory object.
+  // The old object is no longer functional (IsReserved() returns false).
+  void TakeControl(VirtualMemory* from) {
+    ASSERT(!IsReserved());
+    address_ = from->address_;
+    size_ = from->size_;
+    from->Reset();
+  }
+
+  static void* ReserveRegion(size_t size);
+
+  static bool CommitRegion(void* base, size_t size, bool is_executable);
+
+  static bool UncommitRegion(void* base, size_t size, bool is_executable);
+
+  // Must be called with a base pointer that has been returned by ReserveRegion
+  // and the same size it was reserved with.
+  static bool ReleaseRegion(void* base, size_t size);
+
+  // Returns true if OS performs lazy commits, i.e. the memory allocation call
+  // defers actual physical memory allocation till the first memory access.
+  // Otherwise returns false.
+  static bool HasLazyCommits();
+
+  static void SetJITWriteProtect(bool prot);
+
+ private:
+  void* address_;  // Start address of the virtual memory.
+  size_t size_;  // Size of the virtual memory.
+};
 
 // ----------------------------------------------------------------------------
 // OS
@@ -390,10 +483,14 @@ class OS {
 #else
   // Copy memory area to disjoint memory area.
   static void MemCopy(void* dest, const void* src, size_t size) {
+    VirtualMemory::SetJITWriteProtect(false);
     memcpy(dest, src, size);
+    VirtualMemory::SetJITWriteProtect(true);
   }
   static void MemMove(void* dest, const void* src, size_t size) {
+    VirtualMemory::SetJITWriteProtect(false);
     memmove(dest, src, size);
+    VirtualMemory::SetJITWriteProtect(true);
   }
   static const int kMinComplexMemCopy = 16 * kPointerSize;
 #endif  // V8_TARGET_ARCH_IA32
@@ -406,99 +503,6 @@ class OS {
   DISALLOW_IMPLICIT_CONSTRUCTORS(OS);
 };
 
-// Represents and controls an area of reserved memory.
-// Control of the reserved memory can be assigned to another VirtualMemory
-// object by assignment or copy-contructing. This removes the reserved memory
-// from the original object.
-class VirtualMemory {
- public:
-  // Empty VirtualMemory object, controlling no reserved memory.
-  VirtualMemory();
-
-  // Reserves virtual memory with size.
-  explicit VirtualMemory(size_t size);
-
-  // Reserves virtual memory containing an area of the given size that
-  // is aligned per alignment. This may not be at the position returned
-  // by address().
-  VirtualMemory(size_t size, size_t alignment);
-
-  // Releases the reserved memory, if any, controlled by this VirtualMemory
-  // object.
-  ~VirtualMemory();
-
-  // Returns whether the memory has been reserved.
-  bool IsReserved();
-
-  // Initialize or resets an embedded VirtualMemory object.
-  void Reset();
-
-  // Returns the start address of the reserved memory.
-  // If the memory was reserved with an alignment, this address is not
-  // necessarily aligned. The user might need to round it up to a multiple of
-  // the alignment to get the start of the aligned block.
-  void* address() {
-    ASSERT(IsReserved());
-    return address_;
-  }
-
-  // Returns the size of the reserved memory. The returned value is only
-  // meaningful when IsReserved() returns true.
-  // If the memory was reserved with an alignment, this size may be larger
-  // than the requested size.
-  size_t size() { return size_; }
-
-  // Commits real memory. Returns whether the operation succeeded.
-  bool Commit(void* address, size_t size, bool is_executable);
-
-  // Uncommit real memory.  Returns whether the operation succeeded.
-  bool Uncommit(void* address, size_t size);
-
-  // Creates a single guard page at the given address.
-  bool Guard(void* address);
-
-  void Release() {
-    ASSERT(IsReserved());
-    // Notice: Order is important here. The VirtualMemory object might live
-    // inside the allocated region.
-    void* address = address_;
-    size_t size = size_;
-    Reset();
-    bool result = ReleaseRegion(address, size);
-    USE(result);
-    ASSERT(result);
-  }
-
-  // Assign control of the reserved region to a different VirtualMemory object.
-  // The old object is no longer functional (IsReserved() returns false).
-  void TakeControl(VirtualMemory* from) {
-    ASSERT(!IsReserved());
-    address_ = from->address_;
-    size_ = from->size_;
-    from->Reset();
-  }
-
-  static void* ReserveRegion(size_t size);
-
-  static bool CommitRegion(void* base, size_t size, bool is_executable);
-
-  static bool UncommitRegion(void* base, size_t size);
-
-  // Must be called with a base pointer that has been returned by ReserveRegion
-  // and the same size it was reserved with.
-  static bool ReleaseRegion(void* base, size_t size);
-
-  // Returns true if OS performs lazy commits, i.e. the memory allocation call
-  // defers actual physical memory allocation till the first memory access.
-  // Otherwise returns false.
-  static bool HasLazyCommits();
-
- private:
-  void* address_;  // Start address of the virtual memory.
-  size_t size_;  // Size of the virtual memory.
-};
-
-
 // ----------------------------------------------------------------------------
 // Thread
 //
diff --git a/src/spaces-inl.h b/src/spaces-inl.h
index f3ed1bdfb24..98aa307d453 100644
--- a/v8-source/src/spaces-inl.h
+++ b/v8-source/src/spaces-inl.h
@@ -164,6 +164,7 @@ Page* Page::Initialize(Heap* heap,
                        MemoryChunk* chunk,
                        Executability executable,
                        PagedSpace* owner) {
+  VirtualMemory::SetJITWriteProtect(false);
   Page* page = reinterpret_cast<Page*>(chunk);
   ASSERT(page->area_size() <= kMaxRegularHeapObjectSize);
   ASSERT(chunk->owner() == owner);
@@ -171,6 +172,7 @@ Page* Page::Initialize(Heap* heap,
   owner->Free(page->area_start(), page->area_size());
 
   heap->incremental_marking()->SetOldSpacePageFlags(chunk);
+  VirtualMemory::SetJITWriteProtect(true);
 
   return page;
 }
diff --git a/src/spaces.cc b/src/spaces.cc
index cfb70b8fe01..4cd6917061c 100644
--- a/v8-source/src/spaces.cc
+++ b/v8-source/src/spaces.cc
@@ -251,14 +251,14 @@ bool CodeRange::CommitRawMemory(Address start, size_t length) {
 
 
 bool CodeRange::UncommitRawMemory(Address start, size_t length) {
-  return code_range_->Uncommit(start, length);
+  return code_range_->Uncommit(start, length, EXECUTABLE);
 }
 
 
 void CodeRange::FreeRawMemory(Address address, size_t length) {
   ASSERT(IsAddressAligned(address, MemoryChunk::kAlignment));
   free_list_.Add(FreeBlock(address, length));
-  code_range_->Uncommit(address, length);
+  code_range_->Uncommit(address, length, EXECUTABLE);
 }
 
 
@@ -546,7 +546,8 @@ bool MemoryChunk::CommitArea(size_t requested) {
     size_t length = committed_size - commit_size;
     Address start = address() + committed_size + guard_size - length;
     if (reservation_.IsReserved()) {
-      if (!reservation_.Uncommit(start, length)) return false;
+      if (!reservation_.Uncommit(start, length,
+                                 IsFlagSet(IS_EXECUTABLE))) return false;
     } else {
       CodeRange* code_range = heap_->isolate()->code_range();
       ASSERT(code_range->exists() && IsFlagSet(IS_EXECUTABLE));
@@ -787,17 +788,21 @@ bool MemoryAllocator::CommitBlock(Address start,
 }
 
 
-bool MemoryAllocator::UncommitBlock(Address start, size_t size) {
-  if (!VirtualMemory::UncommitRegion(start, size)) return false;
+bool MemoryAllocator::UncommitBlock(Address start, size_t size, Executability executable) {
+  if (!VirtualMemory::UncommitRegion(start, size, executable)) return false;
   isolate_->counters()->memory_allocated()->Decrement(static_cast<int>(size));
   return true;
 }
 
 
 void MemoryAllocator::ZapBlock(Address start, size_t size) {
+  VirtualMemory::SetJITWriteProtect(false);
+
   for (size_t s = 0; s + kPointerSize <= size; s += kPointerSize) {
     Memory::Address_at(start + s) = kZapValue;
   }
+
+  VirtualMemory::SetJITWriteProtect(true);
 }
 
 
@@ -1562,7 +1567,7 @@ bool SemiSpace::Commit() {
 bool SemiSpace::Uncommit() {
   ASSERT(is_committed());
   Address start = start_ + maximum_capacity_ - capacity_;
-  if (!heap()->isolate()->memory_allocator()->UncommitBlock(start, capacity_)) {
+  if (!heap()->isolate()->memory_allocator()->UncommitBlock(start, capacity_, executable())) {
     return false;
   }
   anchor()->set_next_page(anchor());
@@ -1629,7 +1634,7 @@ bool SemiSpace::ShrinkTo(int new_capacity) {
     ASSERT(IsAligned(delta, OS::AllocateAlignment()));
 
     MemoryAllocator* allocator = heap()->isolate()->memory_allocator();
-    if (!allocator->UncommitBlock(start_ + new_capacity, delta)) {
+    if (!allocator->UncommitBlock(start_ + new_capacity, delta, executable())) {
       return false;
     }
 
@@ -2062,6 +2067,7 @@ void FreeListNode::set_next(FreeListNode* next) {
   // While we are booting the VM the free space map will actually be null.  So
   // we have to make sure that we don't try to use it for anything at that
   // stage.
+  VirtualMemory::SetJITWriteProtect(false);
   if (map() == GetHeap()->raw_unchecked_free_space_map()) {
     ASSERT(map() == NULL || Size() >= kNextOffset + kPointerSize);
     Memory::Address_at(address() + kNextOffset) =
@@ -2070,6 +2076,7 @@ void FreeListNode::set_next(FreeListNode* next) {
     Memory::Address_at(address() + kPointerSize) =
         reinterpret_cast<Address>(next);
   }
+  VirtualMemory::SetJITWriteProtect(true);
 }
 
 
@@ -2400,10 +2407,12 @@ HeapObject* FreeList::Allocate(int size_in_bytes) {
   ASSERT(bytes_left >= 0);
 
 #ifdef DEBUG
+  VirtualMemory::SetJITWriteProtect(false);
   for (int i = 0; i < size_in_bytes / kPointerSize; i++) {
     reinterpret_cast<Object**>(new_node->address())[i] =
         Smi::FromInt(kCodeZapValue);
   }
+  VirtualMemory::SetJITWriteProtect(true);
 #endif
 
   // The old-space-step might have finished sweeping and restarted marking.
diff --git a/src/spaces.h b/src/spaces.h
index 770b88a9fba..4f59c2ec518 100644
--- a/v8-source/src/spaces.h
+++ b/v8-source/src/spaces.h
@@ -1154,7 +1154,7 @@ class MemoryAllocator {
   // start is not NULL, the size is greater than zero, and the
   // block is contained in the initial chunk.  Returns true if it succeeded
   // and false otherwise.
-  bool UncommitBlock(Address start, size_t size);
+  bool UncommitBlock(Address start, size_t size, Executability executable);
 
   // Zaps a contiguous block of memory [start..(start+size)[ thus
   // filling it up with a recognizable non-NULL bit pattern.
